#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®˜æ–¹APIæ•°æ®å®Œæ•´å¤„ç†pipeline
Official API Data Processing Pipeline

å°†å®˜æ–¹APIè·å–çš„é¤å…æ•°æ®ä¸ç°æœ‰çš„åˆ†æç³»ç»Ÿé›†æˆ
æ”¯æŒï¼šæ•°æ®è·å– -> æ•°æ®åˆ†æ -> è¯äº‘ç”Ÿæˆ -> ç»“æœå±•ç¤ºçš„å®Œæ•´æµç¨‹
"""

import json
import os
import sys
from datetime import datetime
from typing import Dict, List

# å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from ccc_official_api_client import MultiAPIRestaurantSearcher
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œä½¿ç”¨æœ¬åœ°ç±»å®šä¹‰
    class MultiAPIRestaurantSearcher:
        def __init__(self, api_keys):
            self.api_keys = api_keys

        def search_all_platforms(self, keyword, city, limit_per_platform):
            # è¿”å›æ¼”ç¤ºæ•°æ®
            return {
                'summary': {
                    'keyword': keyword,
                    'city': city,
                    'platforms_used': ['æ¼”ç¤ºå¹³å°'],
                    'total_found': 15
                },
                'restaurants': [
                    {
                        'id': f'demo_{i}',
                        'name': f'{keyword}åº—{i}',
                        'address': f'{city}å¸‚ç¤ºä¾‹åŒºè¡—é“{i}å·',
                        'location': {'lat': 39.9 + i*0.001, 'lng': 116.4 + i*0.001},
                        'phone': f'010-1234{i:04d}',
                        'category': keyword,
                        'rating': 4.0 + (i % 10) * 0.1,
                        'tags': [keyword, 'ç¾é£Ÿ'],
                        'source': 'demo'
                    }
                    for i in range(1, 16)
                ],
                'raw_count': 15,
                'deduplicated_count': 15
            }

        def save_results(self, results, filename=None):
            if not filename:
                from datetime import datetime
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                keyword = results['summary']['keyword'].replace(' ', '_')
                city = results['summary']['city']
                filename = f"data/demo_api_restaurants_{keyword}_{city}_{timestamp}.json"

            import os
            import json
            os.makedirs(os.path.dirname(filename), exist_ok=True)
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            print(f"æ¼”ç¤ºæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return filename
from utils.text_analyzer_simple import CommentAnalyzer
from utils.wordcloud_generator import WordCloudGenerator
from utils.data_utils import DataManager, Logger

class OfficialAPIDataProcessor:
    """å®˜æ–¹APIæ•°æ®å¤„ç†å™¨"""

    def __init__(self, api_keys_file: str = 'api_keys_template.json'):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            api_keys_file: APIå¯†é’¥é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.logger = Logger.setup('api_processor')
        self.data_manager = DataManager()
        self.api_keys = self.load_api_keys(api_keys_file)

        if self.api_keys:
            self.searcher = MultiAPIRestaurantSearcher(self.api_keys)
        else:
            self.searcher = None
            self.logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆAPIå¯†é’¥ï¼Œå°†ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")

    def load_api_keys(self, api_keys_file: str) -> Dict[str, str]:
        """åŠ è½½APIå¯†é’¥"""
        try:
            if not os.path.exists(api_keys_file):
                self.logger.warning(f"APIå¯†é’¥æ–‡ä»¶ä¸å­˜åœ¨: {api_keys_file}")
                return {}

            with open(api_keys_file, 'r', encoding='utf-8') as f:
                config = json.load(f)

            # è¿‡æ»¤æœ‰æ•ˆçš„APIå¯†é’¥
            valid_keys = {}
            for platform, key in config.items():
                if not platform.startswith('_') and key and 'your_' not in key:
                    valid_keys[platform] = key

            if valid_keys:
                self.logger.info(f"æˆåŠŸåŠ è½½APIå¯†é’¥: {list(valid_keys.keys())}")
            else:
                self.logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„APIå¯†é’¥")

            return valid_keys

        except Exception as e:
            self.logger.error(f"åŠ è½½APIå¯†é’¥å¤±è´¥: {e}")
            return {}

    def search_restaurants(self, keyword: str, city: str, limit_per_platform: int = 20) -> str:
        """
        æœç´¢é¤å…æ•°æ®

        Args:
            keyword: æœç´¢å…³é”®è¯
            city: åŸå¸‚åç§°
            limit_per_platform: æ¯ä¸ªå¹³å°çš„ç»“æœæ•°é‡

        Returns:
            æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.logger.info(f"å¼€å§‹æœç´¢é¤å…: {keyword} in {city}")

        if self.searcher:
            # ä½¿ç”¨çœŸå®APIæœç´¢
            results = self.searcher.search_all_platforms(keyword, city, limit_per_platform)
        else:
            # ä½¿ç”¨æ¼”ç¤ºæ•°æ®
            results = self._create_demo_data(keyword, city)

        # ä¿å­˜åŸå§‹APIæ•°æ®
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_keyword = keyword.replace(' ', '_').replace('/', '_')
        api_data_file = f"data/api_restaurants_{safe_keyword}_{city}_{timestamp}.json"

        self.data_manager.save_json(results, api_data_file)
        self.logger.info(f"APIæ•°æ®å·²ä¿å­˜: {api_data_file}")

        return api_data_file

    def _create_demo_data(self, keyword: str, city: str) -> Dict:
        """åˆ›å»ºæ¼”ç¤ºæ•°æ®"""
        self.logger.info("ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®")

        demo_restaurants = []
        for i in range(1, 21):  # ç”Ÿæˆ20å®¶é¤å…
            restaurant = {
                'id': f'demo_{keyword}_{i}',
                'name': f'{keyword}åº—{i}',
                'address': f'{city}å¸‚ç¤ºä¾‹åŒºç¤ºä¾‹è¡—{i}å·',
                'location': {
                    'lat': 39.9 + i * 0.001,
                    'lng': 116.4 + i * 0.001
                },
                'phone': f'010-1234{i:04d}',
                'category': keyword,
                'rating': 3.5 + (i % 10) * 0.15,
                'tags': [keyword, 'ç¾é£Ÿ', 'èšé¤'],
                'source': 'demo',
                'unique_id': f'demo_{i}',
                'demo_reviews': [
                    f'è¿™å®¶{keyword}åº—å‘³é“ä¸é”™ï¼ŒæœåŠ¡å¾ˆå¥½',
                    f'ç¯å¢ƒä¼˜é›…ï¼Œ{keyword}å¾ˆæ­£å®—',
                    f'æ€§ä»·æ¯”å¾ˆé«˜ï¼Œæ¨èè¿™å®¶{keyword}',
                    f'æœ‹å‹èšé¤çš„å¥½åœ°æ–¹ï¼Œ{keyword}å¾ˆæœ‰ç‰¹è‰²',
                    f'ä¸‹æ¬¡è¿˜ä¼šå†æ¥ï¼Œ{keyword}è´¨é‡å¾ˆæ£’'
                ]
            }
            demo_restaurants.append(restaurant)

        return {
            'summary': {
                'keyword': keyword,
                'city': city,
                'timestamp': datetime.now().isoformat(),
                'platforms_used': ['æ¼”ç¤ºæ•°æ®'],
                'total_found': len(demo_restaurants)
            },
            'restaurants': demo_restaurants,
            'raw_count': len(demo_restaurants),
            'deduplicated_count': len(demo_restaurants)
        }

    def convert_to_comments_format(self, api_data_file: str) -> str:
        """
        å°†APIæ•°æ®è½¬æ¢ä¸ºè¯„è®ºåˆ†ææ ¼å¼

        Args:
            api_data_file: APIæ•°æ®æ–‡ä»¶è·¯å¾„

        Returns:
            è¯„è®ºæ ¼å¼æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.logger.info("è½¬æ¢APIæ•°æ®ä¸ºè¯„è®ºåˆ†ææ ¼å¼")

        # åŠ è½½APIæ•°æ®
        api_data = self.data_manager.load_json(api_data_file)
        if not api_data:
            raise ValueError("æ— æ³•åŠ è½½APIæ•°æ®æ–‡ä»¶")

        # è½¬æ¢ä¸ºè¯„è®ºæ ¼å¼
        comments = []
        comment_id = 1

        for restaurant in api_data['restaurants']:
            # åŸºç¡€é¤å…ä¿¡æ¯è½¬æ¢ä¸º"è¯„è®º"
            restaurant_info = {
                'id': f'info_{comment_id}',
                'content': f"é¤å…åç§°ï¼š{restaurant['name']}ï¼Œåœ°å€ï¼š{restaurant['address']}ï¼Œç±»å‹ï¼š{restaurant.get('category', 'é¤å…')}",
                'rating': restaurant.get('rating', 4.0),
                'user_id': 'system_info',
                'timestamp': datetime.now().isoformat(),
                'restaurant_id': restaurant['id'],
                'restaurant_name': restaurant['name'],
                'source': f"å®˜æ–¹API_{restaurant.get('source', 'unknown')}",
                'tags': restaurant.get('tags', [])
            }
            comments.append(restaurant_info)
            comment_id += 1

            # å¦‚æœæœ‰æ¼”ç¤ºè¯„è®ºï¼Œä¹Ÿæ·»åŠ è¿›å»
            if 'demo_reviews' in restaurant:
                for review_text in restaurant['demo_reviews']:
                    review = {
                        'id': f'review_{comment_id}',
                        'content': review_text,
                        'rating': restaurant.get('rating', 4.0) + (-0.5 + comment_id % 3 * 0.5),
                        'user_id': f'user_{comment_id % 100}',
                        'timestamp': datetime.now().isoformat(),
                        'restaurant_id': restaurant['id'],
                        'restaurant_name': restaurant['name'],
                        'source': 'demo_review',
                        'tags': restaurant.get('tags', [])
                    }
                    comments.append(review)
                    comment_id += 1

        # åˆ›å»ºè¯„è®ºæ•°æ®åŒ…
        comments_data = {
            'metadata': {
                'total_comments': len(comments),
                'source': 'official_api',
                'keyword': api_data['summary']['keyword'],
                'city': api_data['summary']['city'],
                'generated_time': datetime.now().isoformat(),
                'api_platforms': api_data['summary']['platforms_used'],
                'data_type': 'restaurant_info_and_reviews'
            },
            'comments': comments
        }

        # ä¿å­˜è¯„è®ºæ ¼å¼æ•°æ®
        comments_file = api_data_file.replace('api_restaurants_', 'comments_')
        self.data_manager.save_json(comments_data, comments_file)
        self.logger.info(f"è¯„è®ºæ ¼å¼æ•°æ®å·²ä¿å­˜: {comments_file}")

        return comments_file

    def run_full_pipeline(self, keyword: str, city: str, limit_per_platform: int = 20) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†pipeline

        Args:
            keyword: æœç´¢å…³é”®è¯
            city: åŸå¸‚åç§°
            limit_per_platform: æ¯ä¸ªå¹³å°çš„ç»“æœæ•°é‡

        Returns:
            å¤„ç†ç»“æœæ‘˜è¦
        """
        self.logger.info("å¼€å§‹è¿è¡Œå®Œæ•´æ•°æ®å¤„ç†pipeline")

        results = {
            'keyword': keyword,
            'city': city,
            'start_time': datetime.now().isoformat(),
            'steps': {},
            'files': {},
            'success': False
        }

        try:
            # æ­¥éª¤1: æœç´¢é¤å…æ•°æ®
            print("ğŸ“¡ æ­¥éª¤ 1/4: æœç´¢é¤å…æ•°æ®...")
            api_data_file = self.search_restaurants(keyword, city, limit_per_platform)
            results['files']['api_data'] = api_data_file
            results['steps']['search'] = {'status': 'success', 'file': api_data_file}
            print("âœ… é¤å…æ•°æ®æœç´¢å®Œæˆ")

            # æ­¥éª¤2: è½¬æ¢æ•°æ®æ ¼å¼
            print("ğŸ”„ æ­¥éª¤ 2/4: è½¬æ¢æ•°æ®æ ¼å¼...")
            comments_file = self.convert_to_comments_format(api_data_file)
            results['files']['comments'] = comments_file
            results['steps']['convert'] = {'status': 'success', 'file': comments_file}
            print("âœ… æ•°æ®æ ¼å¼è½¬æ¢å®Œæˆ")

            # æ­¥éª¤3: æ–‡æœ¬åˆ†æ
            print("ğŸ§  æ­¥éª¤ 3/4: æ‰§è¡Œæ–‡æœ¬åˆ†æ...")
            analysis_file = self.analyze_comments(comments_file)
            results['files']['analysis'] = analysis_file
            results['steps']['analyze'] = {'status': 'success', 'file': analysis_file}
            print("âœ… æ–‡æœ¬åˆ†æå®Œæˆ")

            # æ­¥éª¤4: ç”Ÿæˆè¯äº‘
            print("â˜ï¸ æ­¥éª¤ 4/4: ç”Ÿæˆè¯äº‘...")
            wordcloud_results = self.generate_wordcloud(analysis_file)
            results['files']['wordcloud'] = wordcloud_results
            results['steps']['wordcloud'] = {'status': 'success', 'results': wordcloud_results}
            print("âœ… è¯äº‘ç”Ÿæˆå®Œæˆ")

            results['success'] = True
            results['end_time'] = datetime.now().isoformat()

            print("\n" + "="*60)
            print("ğŸ‰ å®Œæ•´pipelineæ‰§è¡ŒæˆåŠŸ!")
            print("="*60)
            print(f"ğŸ“Š æœç´¢å…³é”®è¯: {keyword}")
            print(f"ğŸ“ ç›®æ ‡åŸå¸‚: {city}")
            print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {comments_file}")
            print(f"ğŸ“ˆ åˆ†ææ–‡ä»¶: {analysis_file}")
            print(f"â˜ï¸ è¯äº‘æ–‡ä»¶: {wordcloud_results.get('overall', {}).get('file', 'N/A') if wordcloud_results else 'N/A'}")
            print("="*60)

        except Exception as e:
            self.logger.error(f"Pipelineæ‰§è¡Œå¤±è´¥: {e}")
            results['error'] = str(e)
            results['success'] = False
            print(f"âŒ Pipelineæ‰§è¡Œå¤±è´¥: {e}")

        return results

    def analyze_comments(self, comments_file: str) -> str:
        """åˆ†æè¯„è®ºæ•°æ®"""
        self.logger.info("å¼€å§‹åˆ†æè¯„è®ºæ•°æ®")

        analyzer = CommentAnalyzer()
        data = self.data_manager.load_json(comments_file)

        if not data:
            raise ValueError("æ— æ³•åŠ è½½è¯„è®ºæ•°æ®")

        # æå–è¯„è®ºå†…å®¹
        comments = data.get('comments', [])
        results = analyzer.analyze_comments(comments)

        # æ·»åŠ å…ƒæ•°æ®
        results['source_metadata'] = data.get('metadata', {})
        results['analysis_time'] = datetime.now().isoformat()
        results['analysis_type'] = 'official_api_data'

        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = comments_file.replace('.json', '_analysis.json')
        self.data_manager.save_json(results, analysis_file)

        self.logger.info(f"åˆ†æç»“æœå·²ä¿å­˜: {analysis_file}")
        return analysis_file

    def generate_wordcloud(self, analysis_file: str) -> Dict:
        """ç”Ÿæˆè¯äº‘"""
        self.logger.info("å¼€å§‹ç”Ÿæˆè¯äº‘")

        generator = WordCloudGenerator()
        analysis_data = self.data_manager.load_json(analysis_file)

        if not analysis_data:
            raise ValueError("æ— æ³•åŠ è½½åˆ†ææ•°æ®")

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ç”Ÿæˆæ•´ä½“è¯äº‘
        keywords = analysis_data.get('keywords', [])
        overall_result = generator.generate_wordcloud(
            keywords=keywords,
            title="å®˜æ–¹APIé¤å…æ•°æ®è¯äº‘å›¾",
            save_path=f"data/api_wordcloud_overall_{timestamp}.png"
        )

        # ç”Ÿæˆåˆ†ç±»è¯äº‘
        category_keywords = analysis_data.get('labels', {}).get('category_keywords', {})
        category_results = generator.generate_category_wordclouds(
            category_keywords,
            save_dir=f"data/api_category_wordclouds_{timestamp}"
        )

        results = {
            'overall': overall_result,
            'categories': category_results,
            'timestamp': timestamp,
            'source': 'official_api_data'
        }

        self.logger.info("è¯äº‘ç”Ÿæˆå®Œæˆ")
        return results

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå®Œæ•´pipeline"""
    print("ğŸª å®˜æ–¹APIé¤å…æ•°æ®å®Œæ•´å¤„ç†pipeline")
    print("æ”¯æŒï¼šæ•°æ®è·å– -> åˆ†æ -> è¯äº‘ç”Ÿæˆ")
    print("="*60)

    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = OfficialAPIDataProcessor()

    # è¿è¡Œæ¼”ç¤ºpipeline
    print("ğŸš€ å¼€å§‹æ¼”ç¤ºå®Œæ•´å¤„ç†æµç¨‹...")

    # ä½¿ç”¨ç¤ºä¾‹å‚æ•°
    keyword = "ç«é”…"
    city = "åŒ—äº¬"

    results = processor.run_full_pipeline(keyword, city, limit_per_platform=10)

    # ä¿å­˜pipelineç»“æœ
    pipeline_file = f"data/pipeline_results_{keyword}_{city}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    processor.data_manager.save_json(results, pipeline_file)

    print(f"\nğŸ“‹ Pipelineç»“æœå·²ä¿å­˜: {pipeline_file}")

    if results['success']:
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®:")
        print("1. ç”³è¯·çœŸå®APIå¯†é’¥ä»¥è·å–å®é™…æ•°æ®")
        print("2. ä¿®æ”¹æœç´¢å‚æ•°ä»¥é€‚åº”æ‚¨çš„éœ€æ±‚")
        print("3. å°†ç»“æœé›†æˆåˆ°æ‚¨çš„åº”ç”¨ä¸­")
    else:
        print(f"\nâŒ Pipelineæ‰§è¡Œå¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")

    return results

if __name__ == "__main__":
    main()