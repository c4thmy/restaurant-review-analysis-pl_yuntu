#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿå¯åŠ¨å’Œæ¼”ç¤ºè„šæœ¬
è§£å†³è·¯å¾„é—®é¢˜å¹¶ç«‹å³å¼€å§‹ä½¿ç”¨ç³»ç»Ÿ
"""

import os
import sys
import json
from datetime import datetime
import subprocess

def show_start_banner():
    """æ˜¾ç¤ºå¯åŠ¨æ¨ªå¹…"""
    print("=" * 60)
    print("    ç«‹å³å¼€å§‹ä½¿ç”¨ - å¤§ä¼—ç‚¹è¯„è¯„è®ºåˆ†æç³»ç»Ÿ")
    print("=" * 60)
    print()

def check_demo_data():
    """æ£€æŸ¥æ¼”ç¤ºæ•°æ®"""
    demo_file = 'data/demo_comments.json'
    if os.path.exists(demo_file):
        print(f"âœ… æ¼”ç¤ºæ•°æ®æ–‡ä»¶å­˜åœ¨: {demo_file}")

        # è¯»å–å¹¶æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
        with open(demo_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        comments = data.get('comments', [])
        print(f"   åŒ…å« {len(comments)} æ¡è¯„è®º")
        print(f"   æ•°æ®ç‰¹ç‚¹: å·²åŒ¿ååŒ–å¤„ç†")
        print()
        return True
    else:
        print(f"âŒ æ¼”ç¤ºæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {demo_file}")
        return False

def run_basic_analysis():
    """è¿è¡ŒåŸºç¡€åˆ†æ"""
    print("ğŸ” å¼€å§‹åŸºç¡€åˆ†ææ¼”ç¤º...")
    print()

    # è¯»å–æ¼”ç¤ºæ•°æ®
    with open('data/demo_comments.json', 'r', encoding='utf-8') as f:
        data = json.load(f)

    comments = data.get('comments', [])

    # åŸºç¡€ç»Ÿè®¡
    total_comments = len(comments)
    ratings = [c.get('rating', 0) for c in comments if c.get('rating')]
    avg_rating = sum(ratings) / len(ratings) if ratings else 0

    # æå–å…³é”®è¯
    all_content = ' '.join([c.get('content', '') for c in comments])

    # ç®€å•è¯é¢‘ç»Ÿè®¡
    words = []
    for comment in comments:
        content = comment.get('content', '')
        # ç®€å•åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
        import re
        comment_words = re.findall(r'[\u4e00-\u9fa5]+', content)
        words.extend(comment_words)

    # ç»Ÿè®¡è¯é¢‘
    from collections import Counter
    word_freq = Counter(words)
    top_words = word_freq.most_common(10)

    # æ ‡ç­¾ç»Ÿè®¡
    all_tags = []
    for comment in comments:
        tags = comment.get('tags', [])
        all_tags.extend(tags)

    tag_freq = Counter(all_tags)

    # æ˜¾ç¤ºåˆ†æç»“æœ
    print("ğŸ“Š åˆ†æç»“æœ:")
    print(f"   æ€»è¯„è®ºæ•°: {total_comments}")
    print(f"   å¹³å‡è¯„åˆ†: {avg_rating:.2f}")
    print(f"   ç‹¬ç«‹ç”¨æˆ·: {len(set(c.get('user_id', '') for c in comments))}")
    print()

    print("ğŸ·ï¸ é«˜é¢‘æ ‡ç­¾:")
    for tag, count in tag_freq.most_common(5):
        print(f"   {tag}: {count}æ¬¡")
    print()

    print("ğŸ“ é«˜é¢‘è¯æ±‡:")
    for word, count in top_words[:8]:
        if len(word) >= 2:  # åªæ˜¾ç¤º2ä¸ªå­—ç¬¦ä»¥ä¸Šçš„è¯
            print(f"   {word}: {count}æ¬¡")
    print()

    # æƒ…æ„Ÿåˆ†æï¼ˆç®€å•ç‰ˆï¼‰
    positive_words = ['å¥½', 'ä¸é”™', 'æ¨è', 'æ–°é²œ', 'å€¼å¾—']
    negative_words = ['è´µ', 'é•¿', 'å·®']

    positive_count = 0
    negative_count = 0

    for comment in comments:
        content = comment.get('content', '')
        for word in positive_words:
            if word in content:
                positive_count += 1
        for word in negative_words:
            if word in content:
                negative_count += 1

    total_sentiment = positive_count + negative_count
    if total_sentiment > 0:
        positive_rate = positive_count / total_sentiment * 100
        print(f"ğŸ˜Š æƒ…æ„Ÿå€¾å‘:")
        print(f"   æ­£é¢è¯„ä»·: {positive_rate:.1f}%")
        print(f"   è´Ÿé¢è¯„ä»·: {100-positive_rate:.1f}%")

    print()

    return {
        'total_comments': total_comments,
        'avg_rating': avg_rating,
        'top_words': top_words,
        'top_tags': tag_freq.most_common(5),
        'positive_rate': positive_rate if total_sentiment > 0 else 0
    }

def create_simple_wordcloud():
    """åˆ›å»ºç®€å•çš„è¯äº‘æ•°æ®"""
    print("â˜ï¸ ç”Ÿæˆè¯äº‘æ•°æ®...")
    print()

    # è¯»å–åˆ†æç»“æœ
    with open('data/demo_comments.json', 'r', encoding='utf-8') as f:
        data = json.load(f)

    comments = data.get('comments', [])

    # æå–æ‰€æœ‰æ–‡æœ¬
    all_text = []
    for comment in comments:
        content = comment.get('content', '')
        tags = comment.get('tags', [])
        all_text.append(content)
        all_text.extend(tags)

    # è¯é¢‘ç»Ÿè®¡
    import re
    from collections import Counter

    words = []
    for text in all_text:
        # æå–ä¸­æ–‡è¯æ±‡
        chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,}', text)
        words.extend(chinese_words)

    word_freq = Counter(words)

    # åˆ›å»ºè¯äº‘æ•°æ®æ ¼å¼
    wordcloud_data = []
    for word, freq in word_freq.most_common(20):
        wordcloud_data.append({
            'name': word,
            'value': freq * 10,  # æ”¾å¤§æ•°å€¼ç”¨äºæ˜¾ç¤º
            'style': {
                'fontSize': min(freq * 5 + 12, 30)
            }
        })

    # ä¿å­˜è¯äº‘æ•°æ®
    wordcloud_file = 'data/wordcloud_data.json'
    with open(wordcloud_file, 'w', encoding='utf-8') as f:
        json.dump(wordcloud_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… è¯äº‘æ•°æ®å·²ä¿å­˜: {wordcloud_file}")
    print("ğŸ“Š è¯äº‘æ•°æ®é¢„è§ˆ:")
    for item in wordcloud_data[:10]:
        print(f"   {item['name']}: {item['value']}")
    print()

    return wordcloud_data

def show_next_steps():
    """æ˜¾ç¤ºåç»­æ­¥éª¤"""
    print("ğŸ¯ åç»­å¯ä»¥å°è¯•:")
    print()
    print("1. æŸ¥çœ‹ç”Ÿæˆçš„åˆ†æç»“æœ:")
    print("   notepad data\\demo_comments.json")
    print("   notepad data\\wordcloud_data.json")
    print()
    print("2. å®‰è£…å®Œæ•´ä¾èµ–åä½¿ç”¨é«˜çº§åŠŸèƒ½:")
    print("   pip install jieba wordcloud matplotlib")
    print("   python ccc-main.py analyze data/demo_comments.json")
    print()
    print("3. å°è¯•çœŸå®æ•°æ®çˆ¬å– (éœ€è¦Chromeæµè§ˆå™¨):")
    print("   python ccc-main.py pipeline \"é¤å…åç§°\" --city åŒ—äº¬ --months 1")
    print()
    print("4. å¯åŠ¨Webç•Œé¢:")
    print("   python ccc-main.py web")
    print()

def demonstrate_privacy_protection():
    """æ¼”ç¤ºéšç§ä¿æŠ¤åŠŸèƒ½"""
    print("ğŸ” éšç§ä¿æŠ¤æ¼”ç¤º:")
    print()

    # è¯»å–æ¼”ç¤ºæ•°æ®
    with open('data/demo_comments.json', 'r', encoding='utf-8') as f:
        data = json.load(f)

    print("âœ… æ•°æ®ä¿æŠ¤æªæ–½:")
    metadata = data.get('metadata', {})

    features = [
        ('éšç§ä¿æŠ¤', metadata.get('privacy_protected', False)),
        ('ç”¨æˆ·åŒ¿ååŒ–', metadata.get('anonymized', False)),
        ('åˆè§„ç‰ˆæœ¬', metadata.get('compliance_version', 'æœªçŸ¥')),
        ('æ¼”ç¤ºæ•°æ®', metadata.get('demo_data', False))
    ]

    for feature, status in features:
        status_text = "âœ… å·²å¯ç”¨" if status else "âŒ æœªå¯ç”¨"
        if isinstance(status, str):
            status_text = f"ç‰ˆæœ¬ {status}"
        print(f"   {feature}: {status_text}")

    print()

    # æ˜¾ç¤ºåŒ¿ååŒ–ç¤ºä¾‹
    sample_comment = data.get('comments', [{}])[0]
    print("ğŸ“ åŒ¿ååŒ–ç¤ºä¾‹:")
    print(f"   ç”¨æˆ·ID: {sample_comment.get('user_id', 'N/A')} (å·²å“ˆå¸ŒåŒ–)")
    print(f"   æ—¶é—´ä¿¡æ¯: {sample_comment.get('time_period', 'N/A')} (å·²æ³›åŒ–)")
    print(f"   å†…å®¹: {sample_comment.get('content', 'N/A')[:30]}...")
    print()

def main():
    """ä¸»å‡½æ•°"""
    show_start_banner()

    # æ£€æŸ¥æ¼”ç¤ºæ•°æ®
    if not check_demo_data():
        print("è¯·å…ˆè¿è¡Œ python test_simple.py åˆ›å»ºæ¼”ç¤ºæ•°æ®")
        return

    # æ¼”ç¤ºéšç§ä¿æŠ¤
    demonstrate_privacy_protection()

    # è¿è¡ŒåŸºç¡€åˆ†æ
    try:
        analysis_result = run_basic_analysis()
        print("âœ… åŸºç¡€åˆ†æå®Œæˆ!")
        print()
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
        return

    # ç”Ÿæˆè¯äº‘æ•°æ®
    try:
        wordcloud_data = create_simple_wordcloud()
        print("âœ… è¯äº‘æ•°æ®ç”Ÿæˆå®Œæˆ!")
        print()
    except Exception as e:
        print(f"âŒ è¯äº‘ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")

    # æ˜¾ç¤ºåç»­æ­¥éª¤
    show_next_steps()

    print("=" * 60)
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼ç³»ç»ŸåŠŸèƒ½æ­£å¸¸è¿è¡Œ")
    print("=" * 60)

if __name__ == '__main__':
    main()